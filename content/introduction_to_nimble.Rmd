---
title: "Introduction to NIMBLE"
subtitle: "enviBayes 2023 NIMBLE short course"
author: "NIMBLE Development Team"
date: "September 2023"
output:
  slidy_presentation: default
  beamer_presentation: default
---
<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE,
                      tidy.opts = list(width.cutoff = 60),
                      tidy = TRUE)
library(coda)
```

# What is NIMBLE?


- A framework for hierarchical statistical models and methods.
- A nearly drop-in alternative to WinBUGS, OpenBUGS and JAGS.
- An extension of the BUGS language for writing new functions and distributions.
- A configurable system for MCMC.
- A library of other methods.
    - Laplace approximation
    - Sequential Monte Carlo (particle filtering) (via the `nimbleSMC` package)
    - Monte Carlo Expectation Maximization (maximum likelihood)
- A model-generic programming system to write new analysis methods.

# When might you use NIMBLE?

<font size="5">
Use cases:

- Experimenting with different samplers.
- Teaching Bayesian/hierarchical modeling algorithms.
- Working with models that have specialized components.
- Working with discrete parameters/latent states.
- Combining standard MCMC samplers with your own specialized sampler.
- Implementing new or modified/extended algorithms (particularly in combination with existing algorithms).
- Using automatic differentiation in algorithms in flexible ways.

Some rough spots:

- Working with models with huge numbers of nodes (in particular huge numbers of parameters that are sampled via individual samplers).
- Algorithms/models requiring parallelization* (beyond simply running multiple chains).
- Algorithms/models requiring use of sparse matrix algebra*. 

(*) Work in progress on this in the new NIMBLE compilation system we've been developing, but this has been a multi-year project...

</font>

# Example: A generalized linear mixed model (GLMM)

* This example is from [Zuur et al. (2009, Mixed Effects Models and Extensions in Ecology with R)](https://www.highstat.com/index.php/mixed-effects-models-and-extensions-in-ecology-with-r) (chapter 13).
* Elaphostrongylus cervi (E. cervi) is a nematode parasite of deer.  On each of 24 farms in Spain, multiple deer were sampled for E. cervi.
* 826 total deer.
* `Sex` (M/F) and (centered) body `Length` are explanatory variables (fixed effects).
* `Farm` is a random effect.
* Response variable is presence (1) or absence (0) of the parasite E. cervi.


# GLMM example: Load data

```{r}
library(nimble)
DeerEcervi <- read.table('DeerEcervi.txt', header = TRUE)
summary(DeerEcervi)

## Create presence/absence data from counts.
DeerEcervi$Ecervi_01 <- DeerEcervi$Ecervi
DeerEcervi$Ecervi_01[DeerEcervi$Ecervi>0] <- 1
## Set up naming convention for centered and uncentered lengths for exercises later
DeerEcervi$unctrLength <- DeerEcervi$Length
## Center Length for better interpretation
DeerEcervi$ctrLength <- DeerEcervi$Length - mean(DeerEcervi$Length)
## Make a factor version of Sex for plotting
DeerEcervi$fSex <- factor(DeerEcervi$Sex)
## Make a factor and id version of Farm
DeerEcervi$fFarm <- factor(DeerEcervi$Farm)
DeerEcervi$farm_ids <- as.numeric(DeerEcervi$fFarm)
```

# GLMM example: Write the model code **in R**

```{r}
DEcode <- nimbleCode({
  for(i in 1:2) {
    # Priors for intercepts and length coefficients for sex = 1 (male), 2 (female)
    sex_int[i] ~ dnorm(0, sd = 1000)
    length_coef[i] ~ dnorm(0, sd = 1000)
  }

  # Priors for farm random effects and their standard deviation.
  farm_sd ~ dunif(0, 20)
  for(i in 1:num_farms) {
    farm_effect[i] ~ dnorm(0, sd = farm_sd)
  }

  # logit link and Bernoulli data probabilities
  for(i in 1:num_animals) {
    logit(disease_probability[i]) <-
      sex_int[ sex[i] ] +
      length_coef[ sex[i] ]*length[i] +
      farm_effect[ farm_ids[i] ]
    Ecervi_01[i] ~ dbern(disease_probability[i])
  }
})
```

# Alternative distribution parameterizations and named parameters

- BUGS/JAGS: Only `dnorm(mu, tau)` is supported, where `tau` is precision. 
- NIMBLE: Alternative parameterizations and named parameters are supported (but defaults are same as with JAGS/WinBUGS):

   
    - `dnorm(mean = mu, sd = sigma)`                    
    - `dnorm(mean = mu, var = sigma_squared)`  
    - `dnorm(mean = mu, tau = tau)` # precision (default if not named)
    
- Distributions with alternative parameterizations are listed in Table 5.2 of [User Manual Section 5.2.4](https://r-nimble.org/html_manual/cha-writing-models.html#subsec:dists-and-functions)

Note we placed the prior directly on the standard deviation parameter. In BUGS or JAGS you would need to do this:

```{r, eval=FALSE}
farm_sd ~ dunif(0, 20)
farm_prec <- 1/farm_sd^2
for(i in 1:num_farms) {
  farm_effect[i] ~ dnorm(0, farm_prec)
}
```

# GLMM Example: Steps to use NIMBLE:


1. Build the model.  It is an R object.
2. Build the MCMC.

    - 2a. Configure the MCMC. (optional)
    - 2b. Customize the MCMC. (optional)
    - 2c. Build the MCMC.

3. Compile the model and MCMC.
4. Run the MCMC.
5. Extract the results.

More about NIMBLE's MCMC workflows is shown in [this figure](./NIMBLE_workflow.html) (also available as [pdf](./NIMBLE_workflow.pdf)).
    
# GLMM Example: 1. Set up the model


```{r}
DEconstants <- list(num_farms = 24,
                    num_animals = 826,
                    length = DeerEcervi$ctrLength,
                    sex = DeerEcervi$Sex,
                    farm_ids = DeerEcervi$farm_ids)

DEdata <- list(Ecervi_01 = DeerEcervi$Ecervi_01)

DEinitialize <- function() {
  list(sex_int = c(0, 0),
       length_coef = c(0, 0),
       farm_sd = 1,
       farm_effect = rnorm(24, 0, 1) )
}

set.seed(123)
DEinits <- DEinitialize()

DEmodel <- nimbleModel(DEcode,
                       constants = DEconstants,
                       data = DEdata,
                       inits = DEinits)
```


We can also set the data and inits after building the model
using `DEmodel$setData` and `DEmodel$setInits`.


# GLMM Example: 2. Build the MCMC

```{r}
DEmcmc <- buildMCMC(DEmodel, enableWAIC = TRUE)
```

# GLMM Example: 3. Compile the model and MCMC.


This can be done in one step or two.  We'll use two.

```{r}
cDEmodel <- compileNimble(DEmodel)
# First call to compileNimble in a session is slower than later calls.
cDEmcmc <- compileNimble(DEmcmc, project = DEmodel)
```

# GLMM Example: 4. Run the MCMC

```{r}
time_baseline <- system.time(DEresults <- runMCMC(cDEmcmc, niter=11000, nburnin=1000, WAIC=TRUE))
cat("Sampling time: ", time_baseline[3], "seconds.\n")
```

# GLMM Example: 5. Extract the samples and WAIC

```{r}
# Samples
samples <- DEresults$samples
```

```{r}
# WAIC (Note: there are different flavors of WAIC that can be chosen earlier.)
WAIC <- DEresults$WAIC
```


# GLMM Example: Results


There are many packages for summarizing and plotting MCMC samples.  NIMBLE does not try to re-invent these wheels.

We'll simply use `coda` and basic graphics in this short course.

```{r, eval = FALSE}
# We haven't provided coda figures, but you can make make them if you want.
library(coda)
pdf("Ecervi_samples_coda.pdf")
plot(as.mcmc(samples))
dev.off()
```


# GLMM Example: Managing the run at the lowest level with `mcmc$run`

Starting from a compiled MCMC, we can also do this:

```{r, eval=FALSE}
cDEmcmc$run(niter = 11000, nburnin=1000)
samples2 <- as.matrix(cDEmcmc$mvSamples)
cDEmcmc$getWAIC()
```


# What are constants? What are data?

### Constants are values needed to define model relationships

- Index ranges like `N`.
- Constant indexing vectors.
- Constants must be provided when creating a model with `nimbleModel`.

### Data represents a flag on the role a node plays in the model

- E.g., data nodes shouldn't be sampled in MCMC.
- Data values can be changed.
- Data can be provided when calling `nimbleModel` or later.

### Providing data and constants together.

- Data and constants can be provided together **as `constants`**.
   - For BUGS/JAGS users, it would be easier to call this "data", but that would blur the concepts.  
- NIMBLE will usually disambiguate data when it is provided as constants.

### What are covariates and other non-parameters/non-observations?

- Covariates/predictors are examples of values that are not parameters nor data in the sense of the likelihood.
- Covariates/predictors can be provided via `constants` if you don't need to change them (often the case).
- Covariates/predictors can be provided via `data` or `inits` if you want to change them.
    - NIMBLE will not treat them as 'data nodes'.

# Indexing

When values are grouped (particularly in irregular ways), we often have (potentially complicated) indexing.

Here `sex` and `farm_ids` are vectors of  membership indexes that are known in advance. Make sure to provide them in `constants` to avoid unnecessary computation.

```{r, eval=FALSE}
for(i in 1:num_animals) {
  logit(disease_probability[i]) <- 
    sex_int[ sex[i] ] +
    length_coef[ sex[i] ] * cLength[i] +
    farm_effect[ farm_ids[i] ]
  Ecervi_01[i] ~ dbern(disease_probability[i])
}
```

Particularly in ecological models, indexing can get very complicated.

The farm grouping above is "ragged" (as is the sex grouping). If the number of observations per farm were constant (aka, "regular", "rectangular"), we could have something like this (ignoring the stratification by sex):

```{r, eval=FALSE}
for(j in 1:num_farms) 
  for(i in 1:num_animals_per_farm) {
    logit(disease_probability[j, i]) <- int + length_coef * cLength[i] + farm_effect[j]
    Ecervi_01[j, i] ~ dbern(disease_probability[j, i])
  }
```

# NIMBLE models as objects

When you create a NIMBLE model, it is an object in R.
The model is a directed acyclic graph (DAG).

You can:

 - Get or set parameter or data values.
 - Determine graph relationships.
 - Calculate log probabilities.
 - Simulate (draw) from distributions.
 - More.

# Linear regression example

Let's use a really simple model:

- Linear regression with 4 data points.

```{r}
set.seed(1)
code <- nimbleCode({
  intercept ~ dnorm(0, sd = 1000)
  slope ~ dnorm(0, sd = 1000)
  sigma ~ dunif(0, 100)
  for(i in 1:4) {
    predicted.y[i] <- intercept + slope * x[i]
    y[i] ~ dnorm(predicted.y[i], sd = sigma)
  }
})
model <- nimbleModel(code, 
                     data = list(y = rnorm(4)),
                     inits = list(intercept = 0.5, 
                                  slope = 0.2, 
                                  sigma = 1,
                                  x = c(0.1, 0.2, 0.3, 0.4)))
```

# Draw the graph

```{r, linmodel-graph, echo = FALSE}
layout <- matrix(ncol = 2, byrow = TRUE,
   # These seem to be rescaled to fit in the plot area,
   # so I'll just use 0-100 as the scale
                 data = c(33, 100,
                          66, 100,
                          50, 0, # first three are parameters
                          15, 50, 35, 50, 55, 50, 75, 50, # x's
                          20, 75, 40, 75, 60, 75, 80, 75, # predicted.y's
                          25, 25, 45, 25, 65, 25, 85, 25) # y's
                 )

sizes <- c(45, 30, 30,
           rep(20, 4),
           rep(50, 4),
           rep(20, 4))

edge.color <- "black"
stoch.color <- "deepskyblue2"
det.color <- "orchid3"
rhs.color <- "gray73"
fill.color <- c(
    rep(stoch.color, 3),
    rep(rhs.color, 4),
    rep(det.color, 4),
    rep(stoch.color, 4)
)

plot(model$graph, vertex.shape = "crectangle",
     vertex.size = sizes,
     vertex.size2 = 20,
     layout = layout,
     vertex.label.cex = 1.0,
     vertex.color = fill.color,
     edge.width = 3,
     asp = 0.5,
     edge.color = edge.color)
```

Think of each line of model code as declaring one or more *nodes*.

# Get and set values

This is done in natural R syntax.
```{r}
model$sigma
model$x
model$x[3] <- 0.6
model$x
```

This can be done with a compiled model too.

# You can also get and set data values

```{r}
model$y
model$y[1] <- 0.8
model$y
```

Useful for simulation studies...

# Querying the graph

Get the names of nodes in the model

```{r}
model$getNodeNames()
```

Get types of nodes

```{r}
model$getNodeNames(dataOnly = TRUE)
```

```{r}
model$getNodeNames(determOnly = TRUE)
```

```{r}
model$isData('y')
model$isData('x')
```

# Querying the graph (cont'd)

Get node relationships

```{r}
model$getDependencies("x[2]")
```

```{r}
model$getDependencies("sigma")
```

```{r}
model$getDependencies("slope")
```

# Why do node relationships matter?

For typical MCMC samplers, `model$getDependencies('slope')` returns the nodes that need to be calculated when sampling (updating) `slope`.

Results from `model$getDependencies` are in *topologically sorted* order:

- If you calculate them in order, you'll get correct results.
- E.g., `predicted.y[2]` comes before `y[2]`.

Consider setting up a Metropolis-Hastings, slice, or Hamiltonian sampler on a subset of nodes in the model. Behind the scenes, `getDependencies` is needed.

Model structure is critical for various other algorithms (e.g., determining latent nodes for SMC or Laplace approximation).

# Nodes vs. variables

In NIMBLE:

- A variable is an object that may contain multiple nodes.  

    - `y` is a variable.

- A node is a part of a variable declared in one line of BUGS code.

    - `y[1]` ... `y[4]` are scalar nodes.

# How vectorizing changes nodes

```{r}
code2 <- nimbleCode({
  intercept ~ dnorm(0, sd = 1000)
  slope ~ dnorm(0, sd = 1000)
  sigma ~ dunif(0, 100)
  predicted.y[1:4] <- intercept + slope * x[1:4] # vectorized node
  for(i in 1:4) {
    # scalar nodes (earlier model version):
    # predicted.y[i] <- intercept + slope * x[i] 
    y[i] ~ dnorm(predicted.y[i], sd = sigma)
  }
})

model2 <- nimbleModel(code2, 
                      data = list(y = rnorm(4)),
                      inits = list(intercept = 0.5, 
                                   slope = 0.2, 
                                   sigma = 1,
                                   x = c(0.1, 0.2, 0.3, 0.4)))
```

# How vectorizing changes nodes (cont'd)

The nodes in the vectorized model

```{r}
model2$getNodeNames()
```

```{r}
model2$getDependencies('x[2]')
```

In this case, if `x[2]` had a prior and was being sampled in MCMC, it would be inefficient to calculate all of `y[1]`, `y[2]`, `y[3]`, `y[4]`.  

# Log probability calculations

```{r}
model2$calculate('y[1:4]')
```

This is the sum of log probabilities of all stochastic nodes in the calculation.

Deterministic nodes have their values calculated but contribute 0 to the log probability.

```{r}
model2$getDependencies('intercept')
model2$calculate(model2$getDependencies('intercept'))
```

In this case, this is the sum of log probabilities from almost the entire model.

Only the priors for `slope` and `sigma` are not included.

# Simulating from the model


In this model, there are no random effects.  The only stochastic nodes are data or parameters with priors.

```{r}
model2$sigma
model2$simulate('sigma')
model2$sigma
```

# Data values are protected from simulation (unless you are sure)



```{r}
model2$y
model2$simulate('y') ## Will not over-write data nodes
model2$y
model2$simulate('y', includeData = TRUE) ## will over-write data nodes
model2$y
```

(The code in these last two slides is not good model-generic programming. More in the next few slides.)

# Understanding *lifted nodes*

Consider the following version of our linear regression model.

`predicted.y[i]` no longer appears.  The expression from that is directly in the `dnorm` for `y[i]`. Also, we now use the variance instead of the standard deviation in the likelihood.

```{r}
code3 <- nimbleCode({
  intercept ~ dnorm(0, sd = 1000)
  slope ~ dnorm(0, sd = 1000)
  sigma2 ~ dinvgamma(1, 1)  # this sort of prior not generally recommended
  for(i in 1:4) {
    y[i] ~ dnorm(intercept + slope * x[i], var = sigma2)
  }
})
model3 <- nimbleModel(code3, 
                      data = list(y = rnorm(4)),
                      inits = list(intercept = 0.5, 
                                   slope = 0.2, 
                                   sigma2 = 1,
                                   x = c(0.1, 0.2, 0.3, 0.4)))
```

# Understanding *lifted nodes* (cont'd)

Look at the nodes:

```{r}
model3$getNodeNames()
```

NIMBLE has created nodes in the role of `predicted.y[i]` and `sigma`.

These are called *lifted nodes*.  They are created by "lifting" an expression out of a parameter for a distribution and creating a deterministic node for that expression.

The purpose of this is to cache values and avoid recalculation when it is not needed.

# Model-generic programming: concrete example

If we change `sigma2` and then directly try to work with `y`, we'll cause (silent errors) because the lifted standard deviation has not been updated.

```{r, lifted}
# example of incorrect method using sigma2
model3$sigma2 <- 100
model3$lifted_sqrt_oPsigma2_cP
model3$simulate('y', includeData = TRUE)
summary(model3$y)
# example of correct method using all node dependencies
depNodes <- model3$getDependencies('sigma2', self = FALSE)
depNodes
model3$simulate(depNodes, includeData = TRUE)
model3$lifted_sqrt_oPsigma2_cP
summary(model3$y)
```

# Model-generic programming

Say we want a function that simulates all parts of a model that depend on some input nodes and then returns the corresponding summed log probability.  I will call this part of the model *downstream*.

```{r, generic-simulate}
simulate_downstream <- function(model, nodes) {
  downstream_nodes <- model$getDependencies(nodes, downstream = TRUE)
  model$simulate( downstream_nodes, includeData = TRUE )
  logProb <- model$calculate( downstream_nodes )
  logProb
}
```
Notice that this function will work with *any* model and *any* set of input nodes.


```{r}
model3$y
simulate_downstream(model3, 'sigma2')
model3$y
```

In this case, the model doesn't have much hierarchical structure.

# Always use graph structure in model-generic programming

You may not know where there are lifted nodes.

Always determine what to `calculate` or `simulate` from `getDependencies` or other such tools.

