---
title: "Introduction to programming with nimbleFunctions"
subtitle: "enviBayes 2023 NIMBLE short course"
author: "NIMBLE Development Team"
date: "September 2023"

output:
  slidy_presentation: default
  beamer_presentation: default
---
<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
eval <- TRUE
library(nimble)
source('setup.R')
```

# Introduction 

*nimbleFunctions* are at the heart of NIMBLE. They are the way that algorithms are implemented. They can also be used for

 - user-defined distributions (already seen),
 - user-defined functions (already seen),
 - user-defined MCMC samplers, and
 - compiling parts of R (not shown), without reference to a model.

But their main purpose is providing a way for developers to implement algorithms.

# Two-stage evaluation in nimbleFunctions


Say we want a nimbleFunction to calculate some nodes and their dependencies.

```{r}
calcDeps <- nimbleFunction(
  setup = function(model, nodes) { # setup function gives first stage of evalution
    calcNodes <- model$getDependencies(nodes)
  },
  run = function() {               # run function (or other methods) give second stage of evaluation
    ans <- model$calculate(calcNodes)
    return(ans)
    returnType(double())
  }
)
```

# Using the nimbleFunction

Let's try this on a very basic linear regression:

```{r}
regrCode <- nimbleCode({
  b0 ~ dnorm(0, sd = 100)
  b1 ~ dnorm(0, sd = 100)
  sigma ~ dunif(0, 100)
  for(i in 1:n)
     y[i] ~ dnorm(b0 + b1*x[i], sd = sigma)
})

set.seed(1)
n <- 100
x <- runif(n)
b0_true <- 0.3
b1_true <- 0.5
sigma_true <- 0.25
y <- rnorm(n, b0_true + b1_true*x, sigma_true)

regrModel <- nimbleModel(regrCode, data = list(y = y), constants = list(n = n, x = x),
                     inits = list(b0 = 1, b1 = 0, sigma = 1), buildDerivs = TRUE)
```

```{r}
calcDeps_regr <- calcDeps(regrModel, c('b0', 'b1', 'sigma'))
calcDeps_regr$run()   ## second stage: run code
```

# Some notes

We could compile `calcDeps_regr`.  Then `run` would become a method of a C++ class.

Notice that:

- `calcDeps_regr` is an R reference class object of a custom-generated class.
- `model` and `calcNodes` are fields in the class

```{r}
class(calcDeps_regr) ## We could have used nimbleFunction's name argument to set the class name
calcDeps_regr$calcNodes[1:20]
```

# Demonstration of uncompiled browsing/debugging.


```{r, eval=FALSE}
calcDeps <- nimbleFunction(
  setup = function(model, nodes) {
    browser()
    calcNodes <- model$getDependencies(nodes)
  },
  run = function() {
    browser()
    ans <- model$calculate(calcNodes)
    return(ans)
    returnType(double())
  }
) ## warning about not being able to compiled with browser() is expected.
```

Stepping through debugging from `browser()` will not work well in Rmarkdown, so this code is not evaluated.  Run it in your own R session.
```{r, eval=FALSE}
calcDeps_regr <- calcDeps(regrModel, c('b0', 'b1', 'sigma')) ## We'll see the setup code followed by internal code.
calcDeps_regr$run()
```

# More about nimbleFunctions

- Without setup code, a `nimbleFunction` becomes an R function (uncompiled) and a C++ function (compiled).
- With setup code, a `nimbleFunction` becomes an R reference class definition (uncompiled) and a C++ class definition (compiled).
    - `nimbleFunction` returns a generator (aka constructor, aka initializer) of new class objects.

# nimbleFunction class definitions (i.e., with setup code):

- `setup` is always executed in R.
    - Typically one-time, high-level processing such as querying model structure.
- `run` and other methods can be run uncompiled (in R) or compiled (via C++).
    - Typically repeated "actual algorithm" calculations such as MCMC sampler updates.
    - Can operate models.
- Any objects (e.g. `calcNodes` and `model`) in `setup` can be used in `run`.
    - Internally, these are automatically set up as class member data.
    - You do not need to explicitly declare class member data.
    - Nodes used in model operations are "baked in" (aka partially evaluated) during compilation. 
        - Node vectors must be created in setup code and used in run code.
        - They can't be dynamically modified in run code.


# Example: using nimbleFunctions for maximization

Suppose we wanted to maximize the likelihood of our regression model.

Let's consider how we would optimize the parameters in a model using a nimbleFunction. Basically, we'll just construct an objective function that we can then pass to R's *optim* function to do the actual numerical optimization. (NIMBLE also has an `optim()` that you can use within a nimbleFunction.)

Plan:

 - Setup code determines what needs to be calculated
 - Run code:
    - checks for correct parameter input length
    - puts parameter values into model
    - calculates the likelihood

# A nimbleFunction for the regression model likelihood

```{r, regr-objective}
objective <- nimbleFunction(
    setup = function(model, nodes) {
        calcNodes <- model$getDependencies(nodes, self = FALSE)
        elements <- model$expandNodeNames(nodes, returnScalarComponents = TRUE)
        n <- length(elements)
    },
    run = function(par = double(1)) {
        returnType(double(0))
        if(length(par) != n)
           stop("Input length does not match number of parameter elements.")
        values(model, nodes) <<- par   # assignment into non-local (nf) variables 
        ans <- model$calculate(calcNodes)  # local assignment
        return(ans)
    }
)
```

As discussed, this is actually a nimbleFunction *generator* -- we can't run it yet -- we need to create a specialized instance of the nimbleFunction that is tailored for some model, in our case the marginalized litters model. 

One caveat: we haven't dealt carefully with the constraint that the standard deviation must be positive.

# Specializing the nimbleFunction to the model

```{r, regr-specialized}
cRegrModel <- compileNimble(regrModel)   # remember to compile model first

rObjective <- objective(regrModel, c('b0', 'b1', 'sigma'))
cObjective <- compileNimble(rObjective, project = regrModel)
```

Now let's try using it (we'll need to ignore the warnings caused by negative sd values).

```{r, regr-optimize}
set.seed(1)
system.time(optR <- optim(c(0, 0, 1), rObjective$run, control = list(fnscale = -1)))
system.time(optC <- optim(c(0, 0, 1), cObjective$run, control = list(fnscale = -1)))
optR
optC
```

# Using derivatives of a model

It's not all that helpful in this small example, but one can provide a gradient function to `optim` to potentially speed up the optimization . Let's use NIMBLE's AD capability to do this.


```{r, regr-objective-grad}
objective_with_grad <- nimbleFunction(
    setup = function(model, nodes) {
        calcNodes <- model$getDependencies(nodes, self = FALSE)
        elements <- model$expandNodeNames(nodes, returnScalarComponents = TRUE)
        n <- length(elements)
        
        # Set up the additional arguments for nimDerivs when the function
        #   called includes model$calculate
        derivsInfo <- makeModelDerivsInfo(model, nodes, calcNodes)
        updateNodes <- derivsInfo$updateNodes
        constantNodes <- derivsInfo$constantNodes
    },
    methods = list(
        fun = function(par = double(1)) {
             returnType(double(0))
             if(length(par) != n)
                stop("Input length does not match number of parameter elements.")
             values(model, nodes) <<- par   # assignment into non-local (nf) variables
             ans <- model$calculate(calcNodes)  # local assignment
             return(ans)
        },
        grad = function(par = double(1)) {
             returnType(double(1))
             d <- nimDerivs(fun(par), wrt = 1:length(par), order = 1,
                 model = model, updateNodes = updateNodes, constantNodes = constantNodes)
             result <- d$jacobian[1, ]
             return(result)
        }
    ), buildDerivs = 'fun'
)
```


```{r, regr-specialized-grad}
rObjective <- objective_with_grad(regrModel, c('b0', 'b1', 'sigma'))
cObjective <- compileNimble(rObjective, project = regrModel)
```

Now let's try using it. We need to use a derivative-based optimization algorithm such as the BFGS (Newton-like) method rather than a derivative-free method like Nelder-Mead.

```{r, regr-optimize-grad}
set.seed(1)
origWarn <- options()$warn
options(warn = -1)   ## turn off the warnings to keep the output cleaner
system.time(optR <- optim(c(0, 0, 1), rObjective$fun, gr = rObjective$grad,
                 method = "BFGS", control = list(fnscale = -1)))
options(warn = origWarn)
system.time(optC <- optim(c(0, 0, 1), cObjective$fun, gr = cObjective$grad,
                 method = "BFGS", control = list(fnscale = -1)))
optR
optC
```

So the speed is not any better than without using the gradient, but notice that fewer function evaluations occurred.

Again, we'd want to deal with the constraint on `sigma`. NIMBLE's AD system provides a convenient way to automatically do transformations to unconstrained spaces.

# Example: Laplace approximation

Laplace approximation:

* data: $y$
* random effect: $x$
* parameters: $\theta$
* Want marginal probability of data: $\int P(y | x, \theta) P(x | \theta) dx$.
* Approximate by:

  * Find $x^{*}$ that maximizes $P(y | x, \theta) P(x | \theta)$ with respect to $x$.
  * Compute the Hessian of $\log(P(y | x, \theta) P(x | \theta))$ at $x = x^{*}$.
  * Do a calculation with $P(y | x^{*}, \theta) P(x^{*} | \theta)$ and the Hessian.
  
# Example: Use Deer E. cervi GLMM

We've used this previously. Here is the code in case we need to look at it:

```{r, eval=FALSE}
DEcode <- nimbleCode({
  for(i in 1:2) {
    # Priors for intercepts and length coefficients for sex = 1 (male), 2 (female)
    sex_int[i] ~ dnorm(0, sd = 1000)
    length_coef[i] ~ dnorm(0, sd = 1000)
  }

  # Priors for farm random effects and their standard deviation.
  farm_sd ~ dunif(0, 20)
  for(i in 1:num_farms) {
    farm_effect[i] ~ dnorm(0, sd = farm_sd)
  }

  # logit link and Bernoulli data probabilities
  for(i in 1:num_animals) {
    logit(disease_probability[i]) <-
      sex_int[ sex[i] ] +
      length_coef[ sex[i] ]*length[i] +
      farm_effect[ farm_ids[i] ]
    Ecervi_01[i] ~ dbern(disease_probability[i])
  }
})

DEconstants <- list(num_farms = 24,
                    num_animals = 826,
                    length = DeerEcervi$ctrLength,
                    sex = DeerEcervi$Sex,
                    farm_ids = DeerEcervi$farm_ids)
```
  
# Example: Laplace approximation

Make Deer E. cervi model with derivative tools built.

```{r, eval=TRUE}
DEmodel <- nimbleModel(DEcode,
                       data = DEdata,
                       constants = DEconstants,
                       inits = DEinits_vals,
                       buildDerivs = TRUE)

cDEmodel <- compileNimble(DEmodel)
```

# Example: Laplace approximation

Here is a nimbleFunction for a basic univariate Laplace approximation.

```{r, eval=eval}
simple_1D_Laplace <- nimbleFunction(
  setup = function(model, RE_node, calc_nodes) {
    derivsInfo <- makeModelDerivsInfo(model, RE_node, calc_nodes)
    updateNodes <- derivsInfo$updateNodes
    constantNodes <- derivsInfo$constantNodes
  },
  methods = list(
    # Calculate -log(P(y | x, theta) P(x | theta)) from input x
    negLogLik = function(x = double(1)) {
      values(model, RE_node) <<- x
      res <- -model$calculate(calc_nodes)
      return(res)
      returnType(double())
    },
    # Get derivatives of -log(P(y | x, theta) P(x | theta))
    derivs_negLogLik = function(x = double(1),
                                order = integer(1)) {
      ans <- derivs(negLogLik(x), wrt = 1:length(x), order = order,
                   updateNodes = updateNodes,
                   constantNodes = constantNodes,
                   model = model)
      return(ans)
      returnType(ADNimbleList())
    },
    # Pull out the gradient of -log(P(y | x, theta) P(x | theta))
    gr_negLogLik = function(x = double(1)) {
      gr <- derivs_negLogLik(x, order = c(1L))
      return(gr$jacobian[1,])
      returnType(double(1))
    },
    # Put the pieces together:
    Laplace = function() {
      # Use correct value of random effect as initial value
      # for optimization.
      x_init <- numeric(length = 1)
      x_init[1] <- model[[RE_node]]
      # This is a minimization of the negative log probability
      optim_result <- optim(par = x_init,
                            fn = negLogLik, gr = gr_negLogLik,
                            method = "BFGS")
      # Get the value at the maximum, x^*
      optim_logLik <- -optim_result$value
      # Get the second derivative at the maximum
      optim_neg_hessian <- derivs_negLogLik(optim_result$par,
                                            order = c(2L))
      log_sigma2 <- -log(optim_neg_hessian$hessian[1,1,1])
      # Calculate the Laplace approximation
      Laplace_approx <- optim_logLik + 0.5 * log_sigma2 + 0.5 * log(2*pi)
      # Leave the model in its original state by replacing the
      # original value and re-calculating.
      model[[RE_node]] <<- x_init[1]
      model$calculate(calc_nodes)
      return(Laplace_approx)
      returnType(double())
    }
  ),
  buildDerivs = 'negLogLik'
)
```

# Example: Laplace approximation

Try the Laplace approximation for the first random effect.

```{r, eval=eval}
RE_node <- "farm_effect[1]"
calc_nodes <- DEmodel$getDependencies(RE_node)
test <- simple_1D_Laplace(DEmodel, RE_node, calc_nodes)
ctest <- compileNimble(test, project = cDEmodel)
ctest$Laplace()
```

# Example: Laplace approximation

What is missing from this simple example?

* We want determination of random effects nodes and calculation nodes to be automatic (but customizable).
* We need to handle scalar or vector random effects.
* We need to maximize the Laplace approximation with respect to the model parameters.  This is a complicated problem because the gradient of the Laplace approximation involves third derivatives.
* We can consider various ways to arrange the different derivatives for efficiency using double-taping schemes.
* We can avoid recalculating the value when obtaining the gradient.
* We need various error trapping and robustness checks.
* We need to handle parameters and/or random effects that have constraints on valid values (e.g. variance > 0).

The full-blown Laplace approximation with all of these features is in the AD version of `nimble`.

* Future work may include different choices of optimization methods.

# Example: Laplace approximation (Skip this slide)

This slide shows use of the fully built Laplace approximation to check the demonstration code above.  We don't need to go through it.

```{r, eval=eval}
param_nodes <- DEmodel$getNodeNames(topOnly = TRUE)
DE_Laplace <- buildLaplace(DEmodel, paramNodes = param_nodes,
                           randomEffectsNodes = RE_node,
                           calcNodes = calc_nodes)
cDE_Laplace <- compileNimble(DE_Laplace, project = DEmodel)
param_values <- values(cDEmodel, param_nodes)
cDE_Laplace$findMLE(param_values)
# See which parameters these correspond to:
param_nodes
```

# Example: Laplace approximation

Use the fully built Laplace approximation, handling all the random effects and maximizing the Laplace-approximated log likelihood.

```{r, eval=eval}
DE_Laplace <- buildLaplace(DEmodel)
cDE_Laplace <- compileNimble(DE_Laplace, project = DEmodel)
LaplaceMLE <- cDE_Laplace$findMLE()
summaryLaplace(cDE_Laplace, LaplaceMLE)
```

# Example: Laplace approximation

Since this is a GLMM, we can compare to a specialized implementation of Laplace approximation.  We'll use `lme4`.

```{r, eval=eval}
library(lme4)
lme4_fit <- glmer(Ecervi_01 ~ ctrLength*fSex + (1 | fFarm), family = binomial, data = DeerEcervi)
summary(lme4_fit)
```

# Example: Laplace approximation

How do the parameters match between `nimble` and `lme4` results?

`lme4` is using treatment contrasts (Sex1 is the "reference" group and Sex2 parameters are differences from Sex1).

* First parameter from `nimble` is the Sex1 intercept, which is the `(Intercept)` from `lme4`.
* Second parameter from `nimble` is the Sex2 intercept, which is `(Intercept)` + `fSex2` from `lme4`.
* Third parameter from `nimble` is the Sex1 coefficient for `ctrLength`.
* Fourth parameter from `nimble` is the Sex2 coefficient for `ctrLength`, which is `ctrLength` + `ctrLength:fSex2` from `lme4`.
* Fifth parameter from `nimble` is the farm-effect standard deviation.

# Design of `nimble`'s MCMC system


Here is a figure of MCMC configuration and MCMCs: [nimble_MCMC_design.pdf](nimble_MCMC_design.pdf)

1. MCMC configuration object: Contains a list of sampler assignments, not actual samplers.
2. MCMC object: Contains a list of sampler objects.

To write new samplers, we need to understand:

- Two-stage evaluation of nimbleFunctions with setup code.
- Setup (configuration) rules for using a new MCMC sampler
- Run-time rules for management of model calculations and saved states.
- More about nimbleFunction programming.

# Example of MCMC configuration object

Look at MCMC configuration object for just a few nodes
```{r}
mcmcConf <- configureMCMC(DEmodel, nodes = 'farm_effect[1:3]')
class(mcmcConf)
ls(mcmcConf)
fe1_sampler <- mcmcConf$getSamplers("farm_effect[1]")[[1]]
class(fe1_sampler)
ls(fe1_sampler)
fe1_sampler$target
fe1_sampler$control
```

# What happens from an MCMC configuration object?


Eventually, an MCMC sampler object is created by a call like this (for adaptive random-walk MH):
```{r, eval=FALSE}
sampler_RW(model, mvSaved, target, control)
```

- This is stage one of two-stage evaluation.  It instantiates an object of a `sampler_RW` class.
- `model` is the model.
- `mvSaved` is a `modelValues` object for keeping a set of saved model states (more later).
- `target` is a vector of target node names.
- `control` is a list of whatever sampler-specific configuration settings are needed.


# More about nimbleFunctions

- Without setup code, a `nimbleFunction` becomes an R function (uncompiled) and a C++ function (compiled).
- With setup code, a `nimbleFunction` becomes an R reference class definition (uncompiled) and a C++ class definition (compiled).
    - `nimbleFunction` returns a generator (aka constructor, aka initializer) of new class objects.

### nimbleFunction class definitions (i.e., with setup code)

- `setup` is always executed in R.
    - Typically one-time, high-level processing such as querying model structure.
- `run` and other methods can be run uncompiled (in R) or compiled (via C++).
    - Typically repeated "actual algorithm" calculations such as MCMC sampler updates.
    - Can operate models.
- Any objects (e.g., `calcNodes` and `model`) in `setup` can be used in `run`.
    - Internally, these are automatically set up as class member data.
    - You do not need to explicitly declare class member data.
    - Nodes used in model operations are "baked in" (aka partially evaluated) during compilation. 
        - Node vectors must be created in setup code and used in run code.
        - They can't be dynamically modified in run code.

# A basic Random-Walk Metropolis-Hastings sampler

```{r}
ourMH <- nimbleFunction(
  name = 'ourMH',                              # Convenient for class name of R reference class and generated C++ class
  contains = sampler_BASE,                     # There is a simple class inheritance system.
  setup = function(model, mvSaved, target, control) {                 # REQUIRED setup arguments
    scale <- if(!is.null(control$scale)) control$scale else 1         # Typical extraction of control choices
    calcNodes <- model$getDependencies(target)                        # Typical query of model structure
  },                                                                  # setup can't return anything
  run = function() {
    currentValue <- model[[target]]                                   # extract current value
    currentLogProb <- model$getLogProb(calcNodes)                     # get log "denominator" from cached values
    proposalValue <- rnorm(1, mean = currentValue, sd = scale)        # generate proposal value
    model[[target]] <<- proposalValue                                 # put proposal value in model
    proposalLogProb <- model$calculate(calcNodes)                     # calculate log "numerator" 
    logAcceptanceRatio <- proposalLogProb - currentLogProb            # log acceptance ratio
    # Alternative:
    # logAcceptanceRatio <- model$calculateDiff(calcNodes)
    accept <- decide(logAcceptanceRatio)                                          # utility function to generate accept/reject decision
    if(accept)                                                        # accept: synchronize model -> mvSaved
      copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    else                                                              # reject: synchronize mvSaved -> model
      copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
  },
  methods = list(                              # required method for sampler_BASE base class
    reset = function() {}
  )
)
```

# Rules for each sampler

### setup function
  
- The four arguments, named exactly as shown, are required.  This allows `buildMCMC` to create any sampler correctly.

### run function

- The `mvSaved` ("modelValues saved") has a saved copy of all model variables and log probabilities
- Upon entry, `run()` can assume:
    - the model is fully calculated (so `getLogProb` and `calculateDiff` make sense).
    - `mvSaved` and the model are synchronized (have the same values).
- Upon exit, `run()` must ensure those conditions are met.
    - That way the next sampler can operator correctly.
- Between entry and exit, `run()` can manipulate the model in any way necessary.

### reset function

- To match the `sampler_BASE` definition, all samplers must have a `reset()` function.


# Stepping through uncompiled execution


Version with `browser()`s

```{r}
ourMH_debug <- nimbleFunction(
  name = 'ourMH_debug',
  contains = sampler_BASE,
  setup = function(model, mvSaved, target, control) {
    browser()
    scale <- if(!is.null(control$scale)) control$scale else 1 
    calcNodes <- model$getDependencies(target)                
  },                                                          
  run = function() {
    browser()
    currentValue <- model[[target]]                           
    currentLogProb <- model$getLogProb(calcNodes)             
    proposalValue <- rnorm(1, mean = currentValue, sd = scale)
    model[[target]] <<- proposalValue                             
    proposalLogProb <- model$calculate(calcNodes)             
    logAcceptanceRatio <- currentLogProb - proposalLogProb    
    accept <- decide(logMHR)                                     
    if(accept)                                                   
      copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    else                                                         
      copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
  },
  methods = list(                              
    reset = function() {}
  )
)
```

# Stepping through uncompiled execution

```{r}
mcmcConf <- configureMCMC(DEmodel, nodes = NULL) ## Make an empty configuration
mcmcConf$addSampler(type = "ourMH_debug", target = 'farm_effect[1]')
```

```{r, eval=FALSE}
# run this on your own to step through in debug (browser) mode.
mcmc <- buildMCMC(mcmcConf)
mcmc$run(5)
```

# A more interesting sampler

Let's work on a more involved case where we can't make such simple modifications to an existing sampler.

We'll put the pieces together for the E. cervi example as follows:

- Use the original parameterization.
- Write a sampler that proposes to add $\delta \sim N(0, \mbox{scale})$ to the two sex-specific intercepts and to subtract the same $\delta$ from every `farm_effect[i]` ($i = 1 \ldots 24$).
- This is a scalar sampler in rotated coordinates.
- The coordinate transformation is linear, so there is no determinant of a Jacobian matrix to incorporate.  In general one needs to be careful to use distribution theory correctly if non-linear coordinate transformations are involved.  We are covering the software, not the math.
- We want proposal scale to be adaptive (self-tuning as the MCMC proceeds).  We will just copy from NIMBLE's `sampler_RW` to implement that.


# Let's make an interesting sampler (2)


```{r}
ourSampler <- nimbleFunction(
  name = 'ourSampler',
  contains = sampler_BASE,
  setup = function(model, mvSaved, target, control) {
    ## control list extraction
    offsetNodes <- control$offsetNodes
    if(is.null(offsetNodes)) stop("Must provide offsetNodes in control list")
    adaptive            <- if(!is.null(control$adaptive))            control$adaptive            else TRUE
    adaptInterval       <- if(!is.null(control$adaptInterval))       control$adaptInterval       else 20 #
    adaptFactorExponent <- if(!is.null(control$adaptFactorExponent)) control$adaptFactorExponent else 0.8
    scale               <- if(!is.null(control$scale))               control$scale               else 1
    ## calculation nodes
    calcNodes <- model$getDependencies(c(target, offsetNodes))
    ## variables for adaptation
    scaleOriginal <- scale
    timesRan      <- 0
    timesAccepted <- 0
    timesAdapted  <- 0
    optimalAR     <- 0.44
    gamma1        <- 0
  },
  run = function() {
    currentTargetValues <- values(model, target)
    currentOffsetNodeValues <- values(model, offsetNodes)
    proposalShift <- rnorm(1, mean = 0, sd = scale)
    proposalTargetValues <- currentTargetValues + proposalShift
    proposalOffsetNodeValues <- currentOffsetNodeValues - proposalShift
    values(model, target) <<- proposalTargetValues
    values(model, offsetNodes) <<- proposalOffsetNodeValues
    logMetropolisHastingsRatio <- calculateDiff(model, calcNodes)
    accept <- decide(logMetropolisHastingsRatio)
    if(accept) nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    else     nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
    if(adaptive)     adaptiveProcedure(accept)
  },
  methods = list(
    adaptiveProcedure = function(accepted = logical()) {
      timesRan <<- timesRan + 1
      if(accepted)     timesAccepted <<- timesAccepted + 1
      if(timesRan %% adaptInterval == 0) {
        acceptanceRate <- timesAccepted / timesRan
        timesAdapted <<- timesAdapted + 1
        gamma1 <<- 1/((timesAdapted + 3)^adaptFactorExponent)
        gamma2 <- 10 * gamma1
        adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
        scale <<- scale * adaptFactor
        timesRan <<- 0
        timesAccepted <<- 0
      }
    },
    reset = function() {
      scale <<- scaleOriginal
      timesRan      <<- 0
      timesAccepted <<- 0
      timesAdapted  <<- 0
      gamma1 <<- 0
    }
  )
)
```

# Let's make an interesting sampler (3)

Configure and build the MCMC.

```{r}
mcmcConf <- configureMCMC(DEmodel)
mcmcConf$addSampler(target = c('sex_int'),
                    type = ourSampler,
                    control = list(offsetNodes = 'farm_effect'))
mcmcConf
mcmc <- buildMCMC(mcmcConf)
```

# Accessing more information from a sampler

One might be interested in  accessing compiled internals.  Here is code from an on-the-fly example.

```{r}
## Make compiled model
cDEmodel <- compileNimble(DEmodel)
## Set internal option to access compiled samplers inside of compiled mcmc below
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
## Compile the MCMC
cmcmc <- compileNimble(mcmc)
## See that the run function is an interface to compiled code via .Call
cmcmc$run
## See where our sampler of interest is in the sampler configuration list
mcmcConf$printSamplers()
## And see that the built and compiled samplerFunctions are in the same order
cmcmc$samplerFunctions

## Run the MCMC (could use runMCMC(cmcmc, ...) instead, but time=TRUE is not available via runMCMC)
cmcmc$run(niter = 1000, time = TRUE)
## Access various internals.  Note that some of these have been reset after adaptation steps.
cmcmc$samplerFunctions[[30]]$timesAdapted
cmcmc$samplerFunctions[[30]]$timesAccepted
cmcmc$samplerFunctions[[30]]$scale
cmcmc$samplerFunctions[[30]]$timesRan
## Look at farm_effect in the compiled model
cDEmodel$farm_effect
## Run our sampler 100 times
for(i in 1:100) cmcmc$samplerFunctions[[30]]$run()
## See if there were any updates
cDEmodel$farm_effect
## See how to look at the mvSaved object from the compiled MCMC
## (There are more direct ways to access values in mvSaved. See modelValues
## documentation.)
## This would show the full object: as.matrix(cmcmc$mvSaved)
## This is what I was looking for in the live session:
cmcmc$mvSaved[['farm_effect']]
## See time spent in each sampler (this will not include the 100 iterations
## we did "by hand", only iterations via cmcmc$run(), or runMCMC, which calls
## cmcmc$run).
cmcmc$getTimes()
```


